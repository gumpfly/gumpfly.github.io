<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Machine Learning," />










<meta name="description" content="The EM algorithm Suppose we have an estimation problem in which we have a training set \(\{x^{(1)},\cdots,x^{(m)}\}\) consisting of \(m\) independent examples. We wish to fit parameters of a model \(p">
<meta name="keywords" content="Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="EM algorithm">
<meta property="og:url" content="http://yoursite.com/2019/12/11/EM-algorithm/index.html">
<meta property="og:site_name" content="gumpfly&#39;s blog">
<meta property="og:description" content="The EM algorithm Suppose we have an estimation problem in which we have a training set \(\{x^{(1)},\cdots,x^{(m)}\}\) consisting of \(m\) independent examples. We wish to fit parameters of a model \(p">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-12-11T12:10:09.678Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="EM algorithm">
<meta name="twitter:description" content="The EM algorithm Suppose we have an estimation problem in which we have a training set \(\{x^{(1)},\cdots,x^{(m)}\}\) consisting of \(m\) independent examples. We wish to fit parameters of a model \(p">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/12/11/EM-algorithm/"/>





  <title>EM algorithm | gumpfly's blog</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?7ee80d489920783329b506e02b7d7604";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">gumpfly's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/11/EM-algorithm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Li Yangang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="gumpfly's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">EM algorithm</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-12-11T20:09:49+08:00">
                2019-12-11
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-comment-o"></i>
              </span>
              
                <a href="/2019/12/11/EM-algorithm/#SOHUCS" itemprop="discussionUrl">
                  <span id="changyan_count_unit" class="post-comments-count hc-comment-count" data-xid="2019/12/11/EM-algorithm/" itemprop="commentsCount"></span>
                </a>
              
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="the-em-algorithm">The EM algorithm</h2>
<p>Suppose we have an estimation problem in which we have a training set <span class="math inline">\(\{x^{(1)},\cdots,x^{(m)}\}\)</span> consisting of <span class="math inline">\(m\)</span> independent examples. We wish to fit parameters of a model <span class="math inline">\(p(x,z)\)</span> to the data, where the likelihood is given by</p>
<p><span class="math display">\[
\begin{align*}
L(\theta)&amp;=log\prod_i^m p(x^{(i)};\theta\\
&amp;=\sum_{i=1}^m logp(x^{(i)};\theta)\\
&amp;=\sum_{i=1}^m log\sum_{z^{(i)}} p(x^{(i)},z^{(i)};\theta)
\end{align*}
\]</span></p>
<p>But, explicitly finding the maximum likelihood estimates of the parameters <span class="math inline">\(\theta\)</span> may be hard. Here, the <span class="math inline">\(z^{(i)}\)</span>’s are the latent random variables; and it is often the case that if the <span class="math inline">\(z^{(i)}\)</span>’s were observed, then maximum likelihood estimation would be easy. In such a setting, the EM algorithm gives an efficient method for maximum likelihood estimation. Maximizing <span class="math inline">\(l(\theta)\)</span> explicitly might be difficult, and our strategy will be instead repeatedly construct a lower-bound on <span class="math inline">\(l(\theta)\)</span> (E-step), and then optimize that lower-bound (M-step). For each <span class="math inline">\(i\)</span>, let <span class="math inline">\(Q_i\)</span> be some distribution over the <span class="math inline">\(z\)</span>’s (<span class="math inline">\(\sum_z Q_i(z)=1,Q_i(z)\ge 0\)</span>).Consider the following:</p>
<p><span class="math display">\[
\begin{align}
L(\theta)&amp;=\sum_{i=1}^mlog\sum_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)\\
&amp;=\sum_{i=1}^mlog\sum_{z^{(i)}}Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)}}\\
&amp;\ge\sum_{i=1}^m\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)}}
\end{align}
\]</span></p>
<p>The last step of this derivation used Jensen’s inequality. Specially, <span class="math inline">\(f(x)=log(x)\)</span> is a convave function, since <span class="math inline">\(f^{&#39;&#39;}(x)=-1/x^2 &lt; 0\)</span> over its domain <span class="math inline">\(x \in \Re^+\)</span>. Also, the term</p>
<p><span class="math display">\[\sum_{z^{(i)}}Q_i(z^{(i)})\left[\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)}}\right]\]</span></p>
<p>in the summation is just an expectation of the quantity <span class="math inline">\([p(x^{(i)},z^{(i)};\theta)/Q_i(z^{(i)})]\)</span>with respect to <span class="math inline">\(z^{(i)}\)</span> drawn according to the distrubtion given by <span class="math inline">\(Q_i\)</span>. By Jensen’s inequality, we have</p>
<p><span class="math display">\[f\left(E_{z^{(i)}\sim Q_i}\left[\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\right]\right)\ge E_{z^{(i)}\sim Q_i}\left[f\left(\frac{p(x^{(i)}),z^{(i)};\theta}{Q_i(z^{(i)})}\right)\right]\]</span></p>
<p>where the “<span class="math inline">\(z^{(i)}\sim Q_i\)</span>” subscripts above indicate that the expectations are with respect to <span class="math inline">\(z^{i}\)</span> drawn from <span class="math inline">\(Q_i\)</span>. This allowed us to go from Equation(2) to Equation(3). Now for any of distributions <span class="math inline">\(Q_i\)</span> the formula (3) gives a lower-bound on <span class="math inline">\(l(\theta)\)</span>. There’re many possible choices for the <span class="math inline">\(Q_i\)</span>’s. Which should we choose? Well, if we have some current guess <span class="math inline">\(\theta\)</span> of the parameters, it seems natural to try to make the lower-bound tight at that value of <span class="math inline">\(\theta\)</span>. I.e., we’ll make the inequality above hold with equality at our particular value of <span class="math inline">\(\theta\)</span>. (We’ll see later how this enables us to prove that <span class="math inline">\(l(\theta)\)</span> increase monotonically with successive iterations of EM.) To make the bound tight for a particular value of <span class="math inline">\(\theta\)</span>, we need for the step involving Jensen’s inequality in our derivation above to hold with equality. For this to be true, we know it is sufficient that the expectation be taken over a “constant”-valued random variable. I.e., we require that</p>
<p><span class="math display">\[\frac{p(x^{(i)}),z^{(i)};\theta}{Q_i(z^{(i)})}=c\]</span></p>
<p>for some constant <span class="math inline">\(c\)</span> that does not depend on <span class="math inline">\(z^{i}\)</span>. This is easily accomplished by choosing</p>
<p><span class="math display">\[Q_i(z^{(i)})\propto p(x^{(i)},z^{(i)};\theta)\]</span></p>
<p>Actually, since we know <span class="math inline">\(\sum_z Q_i(z^{(i)})=1\)</span> (because it is a distribution), this further tells us that</p>
<p><span class="math display">\[
\begin{align*}
Q_i(z^{i})&amp;=\frac{p(x^{(i)},z^{(i)};\theta)}{\sum_z p(x^{(i)},z^{(i)};\theta)}\\
&amp;=\frac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)}\\
&amp;=p(z^{(i)}|x^{(i)};\theta)
\end{align*}
\]</span></p>
<p>Thus, we simply set the <span class="math inline">\(Q_i\)</span>’s to be the posterior distribution of the <span class="math inline">\(z^{(i)}\)</span>’s given <span class="math inline">\(x^{(i)}\)</span> and the setting of the parameters <span class="math inline">\(\theta\)</span>. Now, for this choice of the <span class="math inline">\(Q_i\)</span>’s, Equation(3) gives a lower-bound on the loglikelihood <span class="math inline">\(l\)</span> that we’re trying to maximize. This is the E-step. In the M-step of the algorithm, we then maximize our formula in Equation (3) with respect to the parameters to obtain a new setting of the <span class="math inline">\(\theta\)</span>’s. Repeatedly carrying out these steps gives us the EM algorithm, which is as follows: Repeat until convergence{<br>
(E-step) For each <span class="math inline">\(i\)</span>, set <span class="math display">\[Q_i(z^{(i)}):=p(z^{(i)}|x^{(i)};\theta)\]</span> (M-step) Set <span class="math display">\[\theta := \mathop{\arg\max}_{\theta} \sum_i \sum_{z^{(i)}} Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\]</span><br>
}<br>
How we know if this algorithm will converge? Well, suppose <span class="math inline">\(\theta^{(t)}\)</span> and <span class="math inline">\(\theta^{(t+1)}\)</span> are the parameters from two successive iterations of EM. We will now prove that <span class="math inline">\(l(\theta^{(t)})\le l(\theta^{(t+1)})\)</span>, which shows EM always monotonically improves the log-likelihood. The key to showing this result lies in our choice of the <span class="math inline">\(Q_i\)</span>’s. Specially, on the iteration of EM in which the parameters had started out as <span class="math inline">\(\theta^{(t)}\)</span>, we would have chosen <span class="math inline">\(Q_i^{(t)}(z^{(i)}):=p(z^{(i)}|x^{(i)};\theta^{(t)}).\)</span> We saw earlier that this choice ensures that Jensen’s inequality, as applied to get Equation (3), holds with equality, and hence</p>
<p><span class="math display">\[l(\theta^{(t)})=\sum_i \sum_{z^{(i)}} Q_i^{(t)}(z^{(i)})log \frac{p(x^{(i)},z^{(i)};\theta^{(t)})}{Q_i^{(t)}(z^{(i)})}\]</span> The parameters <span class="math inline">\(\theta^{(t+1)}\)</span> are then obtained by maximizing the right hand side of the equation above. Thus,</p>
<p><span class="math display">\[
\begin{align}
l(\theta^{(t+1)}) &amp;\ge \sum_i \sum_{z^{(i)}} Q_i^{(t)}(z^{(i)})log \frac{p(x^{(i)},z^{(i)};\theta^{(t+1)})}{Q_i^{(t)}(z^{(i)})}\\
&amp;\ge \sum_i \sum_{z^{(i)}} Q_i^{(t)}(z^{(i)})log \frac{p(x^{(i)},z^{(i)};\theta^{(t)})}{Q_i^{(t)}(z^{(i)})}\\
&amp;=l(\theta^{(t)})
\end{align}
\]</span></p>
<p>This first inequality comes from the fact that</p>
<p><span class="math display">\[l(\theta) \ge \sum_i \sum_{z^{(i)}} Q_i(z^{(i)})log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\]</span></p>
<p>holds for any values of <span class="math inline">\(Q_i\)</span> and <span class="math inline">\(\theta\)</span>, and in particular holds for <span class="math inline">\(Q_i=Q_i^{(t)}\)</span>, <span class="math inline">\(\theta = \theta^{(t+1)}\)</span>. To get Equation (5), we used the fact that <span class="math inline">\(\theta^{(t+1)}\)</span> is chosen explicitly to be</p>
<p><span class="math display">\[\mathop{\arg\max}_{\theta} \sum_i \sum_{z^{(i)}} Q_i(z^{(i)})log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\]</span></p>
<p>and thus this formula evaluated at <span class="math inline">\(\theta^{(t+1)}\)</span> must be equal to or larger than the same formula evaluated at <span class="math inline">\(\theta^{(t)}\)</span>. Finally, the step used to get (6) was shown earlier, and follows from <span class="math inline">\(Q_i^{(t)}\)</span> having been chosen to make Jensen’s inequality hold with equality at <span class="math inline">\(\theta^{(t)}\)</span>. Hence, EM causes the likelihood to converge monotonically. In our description of the EM algorithm, we said we’d run it until convergence. Given the result that we just showed, one reasonable convergence test would be to check if the increase in <span class="math inline">\(l(\theta)\)</span> between successive iterations is smaller than some tolerance parameter, and to declare convergence if EM is improving <span class="math inline">\(l(\theta)\)</span> two slowly.</p>
<p>Remark. if we define</p>
<p><span class="math display">\[J(Q,\theta) = \sum_i \sum_{z^{(i)}} Q_i(z^{(i)})log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(    i)})}\]</span></p>
<p>then we know <span class="math inline">\(l(\theta) \ge J(Q,\theta)\)</span> from our previous derivation. The EM can also be viewed a coordinate ascent on J, in which the E-step maximizes it with respect to Q, and the M-step maximizes it with respect to <span class="math inline">\(\theta\)</span>.</p>
<h2 id="mixture-of-gaussians">Mixture of Gaussians</h2>
<p>　　Suppose that we are given a training set <span class="math inline">\(\{x^{(1)},\cdots,x^{(m)}\}\)</span> as usual. Since we are in the unsupervised learning setting, these points do not come with any labels.<br>
　　We wish to model the data by specifying a joint distribution <span class="math inline">\(p(x^{(i)},z^{(i)})=p(x^{(i)}|z^{(i)})p(z^{(i)})\)</span>. Here, <span class="math inline">\(z^{(i)}\sim\)</span> Multinomial(<span class="math inline">\(\phi\)</span>) (where <span class="math inline">\(\phi \ge 0,\sum_{j=1}^k \phi_j = 1\)</span>, and the parameter <span class="math inline">\(\phi_j\)</span> gives <span class="math inline">\(p(z^{(i)}=j)\)</span>), and <span class="math inline">\(x^{(i)}|z^{(i)}=j \sim \mathcal{N}(\mu_j,\Sigma_j)\)</span>. We let <span class="math inline">\(k\)</span> denote the number of values that the <span class="math inline">\(z^{(i)}\)</span>’s can take on. Thus, our model posits the each <span class="math inline">\(x^{(i)}\)</span> was generated by randomly choosing <span class="math inline">\(z^{(i)}\)</span> from <span class="math inline">\(\{1,\cdots,k\}\)</span>, and then <span class="math inline">\(x^{(i)}\)</span> was generated by randomly choosing <span class="math inline">\(z^{(i)}\)</span> from <span class="math inline">\({1,\cdots,k}\)</span>, and then <span class="math inline">\(x^{(i)}\)</span> was drawn from one of <span class="math inline">\(k\)</span> Gaussains depending on <span class="math inline">\(z^{(i)}\)</span>. This is called the mixture of Gaussians model. Also, note that the <span class="math inline">\(z^{(i)}\)</span>’s are latent random variables, meaning that they’re hidden/unobserved. This is what will make our estimation problem difficult.<br>
　　The parameters of our model are thus <span class="math inline">\(\phi,\mu\)</span> and <span class="math inline">\(\Sigma\)</span>. To estimate them, we can wirte down the likelihood of our data:<br>
<span class="math display">\[
\begin{align*}
l(\phi,\mu,\Sigma)&amp;=\sum_{i=1}^mlogp(x^{(i)};\phi,\mu,\Sigma)\\
&amp;=\sum_{i=1}^mlog\sum_{z^{(i)}=1}^k p(x^{(i)}|z^{(i)};\mu,\Sigma)p(z^{(i)};\phi)
\end{align*}
\]</span> 　　However, if we set to zero the derivatives of this formula with respect to the parameters and try to solve, we’ll find that it is not possible to find the maximum likelihood estimates of the parameters in closed form.<br>
　　The random variables <span class="math inline">\(z^{(i)}\)</span> indicate which of the <span class="math inline">\(k\)</span> Gaussians each <span class="math inline">\(x^{(i)}\)</span> had come from. Note that if we knew what the <span class="math inline">\(z^{(i)}\)</span>’s were, the maximum likelihood problem would have been easy. Specifically, we could then write down the likelihood as<br>
<span class="math display">\[l(\phi,\mu,\Sigma)=\sum_{i=1}^mlogp(x^{(i)}|z^{(i)};\mu,\Sigma)+logp(z^{(i)};\phi)\]</span> Maximizing this with respect to <span class="math inline">\(\phi,\mu\)</span> and <span class="math inline">\(\Sigma\)</span> gives the parameters:</p>
<p><span class="math display">\[
\begin{align*}
\phi_j &amp;= \frac{1}{m} \sum_{i=1}^m 1\{z^{(i)}=j\}\\
\mu_j &amp;= \frac{\sum_{i=1}^m 1\{z^{(i)}=j\}x^{(i)}}{\sum_{i=1}^m 1\{z^{(i)}=j\}}\\
\Sigma_j &amp;= \frac{\sum_{i=1}^m 1\{z^{(i)}=j\}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^m 1\{z^{(i)}=j\}}
\end{align*}
\]</span> Indeed, we see that if the <span class="math inline">\(z^{(i)}\)</span>’s were known, then maximum likelihood estimation becomes nearly identical to what we had when estimating the parameters of the Gaussian discriminat analysis model, except that here the <span class="math inline">\(z^{(i)}\)</span>’s playing the role of the class labels.<br>
　　However, in our density estimation problem, the <span class="math inline">\(z^{(i)}\)</span>’s not known.What can we do?<br>
　　The EM algorithm is an iterative algorithm that has two main steps. Applied to our problem, in the E-step, it tries to “guess” the values of the <span class="math inline">\(z^{(i)}\)</span>’s. In the M-step, it updates the parameters of our model based on our guesses. Since in the M-step we are pretending that the guesses in the first part were correct, the maximization becomes easy. Here’s the algorithm:<br>
　　Repeat until convergence: {<br>
　　　(E-step) For each <span class="math inline">\(i,j\)</span>, set<br>
<span class="math display">\[w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)\]</span> 　　　(M-step) Update the parameters:<br>
<span class="math display">\[
\begin{align*}
\phi_j &amp;:= \frac{1}{m}\sum_{i=1}^m w_j^{(i)}\\
\mu_j &amp;:= \frac{\sum_{i=1}^m w_j^{(i)}x^{(i)}}{\sum_{i=1}^m w_j^{(i)}}\\
\Sigma_j &amp;:= \frac{\sum_{i=1}^m w_j^{(i)}(x^{(i)}-u_j)(x^{(i)}-u_j)^T}{\sum_{i=1}^m w_j^{(i)}}
\end{align*}
\]</span></p>
<p>　　In the E-step, we calculate the posterior probability of our parameters the <span class="math inline">\(z^{(i)}\)</span>’s, given the <span class="math inline">\(x^{(i)}\)</span> and using the current setting of our parameters. I.e., using Bayes rule,we obtain:</p>
<p><span class="math display">\[p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)=\frac{p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{\sum_{l=1}^k p(x^{(i)}|z^{(i)}=l;\mu,\Sigma)p(z^{(i)}=l;\phi)}\]</span></p>
<p>　　Here, <span class="math inline">\(p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)\)</span> is given by evaluating the density of a Gaussian with mean <span class="math inline">\(\mu_j\)</span> and covariance <span class="math inline">\(\Sigma_j\)</span> at <span class="math inline">\(x^{(i)}\)</span>; <span class="math inline">\(p(z^{(i)}=j;\phi)\)</span> is given by <span class="math inline">\(\phi_j\)</span>, and so on. The values <span class="math inline">\(w_j^{(i)}\)</span> calculated in the E-step represent our “soft” guesses for the value of <span class="math inline">\(z^{(i)}\)</span>.<br>
　　Also, you should constract the update in the M-step with the formulas we had when the <span class="math inline">\(z^{(i)}\)</span>’s were known exactly. They are identical, except that instead of the indicator functions “<span class="math inline">\(1\{z^{(i)}=j\}\)</span>” indicating from which Gaussian each datapoint had come, we now instead have the <span class="math inline">\(w_j^{(i)}\)</span>’s.<br>
　　The EM-algorithm is also reminiscent of the K-means clustering algorithm, expcept instead of the “hard” cluster assignments <span class="math inline">\(c(i)\)</span>, we instead have the “soft” assignments <span class="math inline">\(w_j^{(i)}\)</span>. Similar to K-means, it is also susceptible to local optima, so reinitializing at several different initial parameters may be a good idea.</p>
<p>Armed with our general definition of the EM algorithm, let’s go back to the example of fitting the parameters <span class="math inline">\(\phi\)</span>, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span> in a mixture of Gaussians. The E-step is easy. Following our algorithm derivation above, we simply calculate</p>
<p><span class="math display">\[w_j^{(i)}=Q_i(z^{(i)}=j)=P(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma).\]</span></p>
<p>Here, “<span class="math inline">\(Q_i(z^{(i)}=j)\)</span>” denotes the probability of <span class="math inline">\(z^{(i)}\)</span> taking the value <span class="math inline">\(j\)</span> under the distribution <span class="math inline">\(Q_i\)</span>. Next, in the M-step, we need to maximize, with respect to our parameters <span class="math inline">\(\phi,\mu,\Sigma,\)</span> the equality <span class="math display">\[
\begin{align}
 \sum_i \sum_{z^{(i)}} Q_i(z^{(i)})&amp;log \frac{p(x^{(i)},z^{(i)};\phi,\mu,\Sigma)}{Q_i(z^{(i)})}\\
&amp;= \sum_{i=1}^m \sum_{z^{(i)}} Q_i(z^{(i)}=j)log \frac{p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{Q_i(z^{(i)}=j)}\\
&amp;=\sum_{i=1}^m \sum_{j=1}^k w_j^{(i)}log\frac{\frac{1}{(2\pi)^{n/2}|\Sigma_j|^{1/2}}exp(-\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j))\dot\phi_j}{w_j^{(i)}}
\end{align}
\]</span></p>
<p>Let’s maximize this with respect to <span class="math inline">\(\mu_l\)</span>. If we take the derivative with respect to <span class="math inline">\(\mu_l\)</span>, we find</p>
<p><span class="math display">\[
\begin{align}
\nabla_{\mu_l} \sum_{i=1}^m \sum_{j=1}^k w_j^{(i)}&amp;log\frac{\frac{1}{(2\pi)^{n/2}|\Sigma_j|^{1/2}}exp(-\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j))\dot\phi_j}{w_j^{(i)}}\\
&amp;= -\nabla_{\mu_l} \sum_{i=1}^m \sum_{j=1}^k w_j^{(i)}\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j))\\
&amp;=\frac{1}{2}\sum_{i=1}^m w_l^{(i)}\nabla_{\mu_l}2\mu_l^T\Sigma_l^{-1}x^{(i)}-\mu_l^T\Sigma_l^{-1}\mu_l\\
&amp;=\sum_{i=1}^m w_l^{(i)}(\Sigma_l^{-1}x^{(i)}-\Sigma_l^{-1}\mu_l)
\end{align}
\]</span></p>
<p>setting this to zero and solving for <span class="math inline">\(\mu_l\)</span> therefor yields the updata rule</p>
<p><span class="math display">\[\mu_l := \frac{\sum_{i=1}^m w_l^{(i)}x^{(i)}}{\sum_{i=1}^m w_l^{(i)}}\]</span></p>
<p>Let’s do one more example, and derive the M-step updata for the parameters <span class="math inline">\(\phi_j\)</span>. Grouping together only the terms that depend on <span class="math inline">\(\phi_j\)</span>, we find that we need to maximize</p>
<p><span class="math display">\[\sum_{i=1}^m \sum_{j=1}^k w_j^{(i)}log\phi_j\]</span></p>
<p>However, there is an additional constraint that the <span class="math inline">\(\phi_j\)</span>’s sum to 1, since they represent the probabilities <span class="math inline">\(\phi_j = p(z^{(i)}=j;\phi)\)</span>. To deal with the constraint that <span class="math inline">\(\sum_{j=1}^k \phi_j = 1\)</span>, we construct the Lagrangian</p>
<p><span class="math display">\[L(\phi)=\sum_{i=1}^m \sum_{j=1}^k w_j^{(i)}log\phi_j + \beta(\sum_{j=1}^k \phi_j-1),\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is the Lagrange multiplier. Taking derivatives, we find</p>
<p><span class="math display">\[\frac{\partial}{\partial\phi_j}L(\phi)=\sum_{i=1}^m\frac{w_j^{(i)}}{\phi_j}+1\]</span></p>
<p>Setting this to zero and solving, we get</p>
<p><span class="math display">\[\phi_j = \frac{\sum_{i=1}^m w_j^{(i)}}{-\beta}\]</span></p>
<p>I.e., <span class="math inline">\(\phi_j\propto \sum_{i=1}^m w_j^{(i)}\)</span>.Using the constraint that <span class="math inline">\(\sum_j\phi_j=1\)</span>, we easily find that <span class="math inline">\(-\beta = \sum_{i=1}^m \sum_{j=1}^k w_j^{(i)}=\sum_{i=1}^m 1 = m\)</span>. (This used the fact that <span class="math inline">\(w_j^{(i)}=Q_i(z^{(i)}=j)\)</span>, and since probabilities sum to 1, <span class="math inline">\(\sum_j w_j^{(i)}=1\)</span>.) We therefore have our M-step updates for the parameter <span class="math inline">\(\phi_j\)</span>:</p>
<p><span class="math display">\[\phi_j := \frac{1}{m}\sum_{i=1}^m w_j^{(i)}\]</span></p>
<p>The derivation for M-step updates to <span class="math inline">\(\Sigma_j\)</span> are also entirely straightforward.Look at it as below. <span class="math display">\[
\begin{align*}
\nabla_{\Sigma_l} \sum_{i=1}^m \sum_{j=1}^k w_j^{(i)}&amp;log\frac{\frac{1}{(2\pi)^{n/2}|\Sigma_j|^{1/2}}exp(-\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j))\dot\phi_j}{w_j^{(i)}}\\
&amp;=\nabla_{\Sigma_l}\sum_{i=1}^m\sum_{j=1}^k w_j^{(i)}\left\{-\frac{1}{2}log |\Sigma_j|-\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j)\right\}\\
&amp;=\nabla_{\Sigma_l}\sum_{i=1}^m w_l^{(i)}\left\{log|\Sigma_l|+(x^{(i)}-\mu_l)^T\Sigma_l^{-1}(x^{(i)}-\mu_l)\right\}
\end{align*}
\]</span> The <span class="math inline">\(|S|\)</span> is the determinant of <span class="math inline">\(S\)</span>, we express it as <span class="math inline">\(det(S)\)</span>. Let <span class="math inline">\(S=\Sigma\)</span>, <span class="math inline">\(y_i=x^{(i)}-\mu_l\)</span>, <span class="math inline">\(J(S)=\sum_{i=1}^m w^{(i)}\left\{log det(S)-y_i^TS^{-1}y_i\right\}, (w^{(i)}&gt;0, y_i \in \mathbb{R}^m, S\in \mathbb{R}^{m \times m})\)</span>.<br>
We know that <span class="math inline">\(det(S)I = S Adj(S)\)</span>, <span class="math inline">\(\frac{\partial det(S)}{\partial S}=det(S)S^{-1}\)</span>,<span class="math inline">\((S^{-1})^{\prime}=-S^{-1}S^{\prime}S^{-1}\)</span>, <span class="math inline">\(\frac{\partial S}{\partial S}=I\)</span>, you’ll find<br>
<span class="math display">\[ \frac{\partial log det(S)}{\partial S}=\frac{det(S)S^{-1}}{det(S)}=S^{-1}\]</span> <span class="math display">\[\frac{\partial J(S)}{\partial S}=\sum_{i=1}^m w^{(i)}S^{-1}-\sum_{i=1}^m w^{(i)}y_iy_i^T(-S^{-1}IS^{(-1)})=0\]</span> <span class="math display">\[
\begin{align*}
\sum_{i=1}^m w^{(i)}S^{-1}&amp;=\sum_{i=1}^m w^{(i)}y_iy_i^TS^{-1}IS^{(-1)}\\
\sum_{i=1}^m w^{(i)}&amp;=\sum_{i=1}^m w^{(i)}y_iy_i^TS^{-1}\\
S&amp;=\frac{\sum_{i=1}^m w^{(i)}y_iy_i^T}{\sum_{i=1}^m w^{(i)}}
\end{align*}
\]</span></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
        </div>
      

      
      
      

      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="SOHUCS"></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Li Yangang" />
            
              <p class="site-author-name" itemprop="name">Li Yangang</p>
              <p class="site-description motion-element" itemprop="description">情贵淡，气贵和。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#the-em-algorithm"><span class="nav-number">1.</span> <span class="nav-text">The EM algorithm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mixture-of-gaussians"><span class="nav-number">2.</span> <span class="nav-text">Mixture of Gaussians</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li Yangang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  









  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three-waves.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  




  
    <script type="text/javascript">
    (function(){
      var appid = 'cytMIHrjF';
      var conf = '5f777710b2378182ffe0b2ec8de599d4';
      var width = window.innerWidth || document.documentElement.clientWidth;
      if (width < 960) {
      window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>'); } else { var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){
        window.changyan.api.config({appid:appid,conf:conf})});
      }
    })();
    </script>
    <script type="text/javascript" src="https://assets.changyan.sohu.com/upload/plugins/plugins.count.js"></script>
  









  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        },
        TeX: {equationNumbers: { autoNumber: "AMS" }}
      });
    </script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
