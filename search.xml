<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[c]]></title>
    <url>%2F2018%2F10%2F25%2Fc%2F</url>
    <content type="text"><![CDATA[固定分配数组两种方式 1.常量 123const int M=100;const int N=200;int arr[M][N]; 2.宏定义 123#define M 100#define N 200int arr[M][N] 动态分配数组两种方式 1.二维数组内的空间不连续 12345int M=100;int N=200;int **arr=new int*[M];for(int i=0;i&lt;M;i++) arr[i]=new int[N]; 释放数组内存代码： 123for(int i=0;i&lt;M;i++) delete [] arr[i];delete [] arr; 2.二维数组存放在一组连续空间内 123456int M=100;int N=200;int **arr = new int*[M];int *buffer=new int[M*N];for(int i=0;i&lt;M;i++) arr[i]=buffer+i*N; 释放数组内存代码： 12delete [] buffer;delete [] arr;]]></content>
  </entry>
  <entry>
    <title><![CDATA[EM algorithm]]></title>
    <url>%2F2018%2F09%2F05%2FEM-algorithm%2F</url>
    <content type="text"><![CDATA[The EM algorithm Suppose we have an estimation problem in which we have a training set \(\{x^{(1)},\cdots,x^{(m)}\}\) consisting of \(m\) independent examples. We wish to fit parameters of a model \(p(x,z)\) to the data, where the likelihood is given by \[ \begin{align*} L(\theta)&amp;=log\prod_i^m p(x^{(i)};\theta\\ &amp;=\sum_{i=1}^m logp(x^{(i)};\theta)\\ &amp;=\sum_{i=1}^m log\sum_{z^{(i)}} p(x^{(i)},z^{(i)};\theta) \end{align*} \] But, explicitly finding the maximum likelihood estimates of the parameters \(\theta\) may be hard. Here, the \(z^{(i)}\)'s are the latent random variables; and it is often the case that if the \(z^{(i)}\)'s were observed, then maximum likelihood estimation would be easy. In such a setting, the EM algorithm gives an efficient method for maximum likelihood estimation. Maximizing \(l(\theta)\) explicitly might be difficult, and our strategy will be instead repeatedly construct a lower-bound on \(l(\theta)\) (E-step), and then optimize that lower-bound (M-step). For each \(i\), let \(Q_i\) be some distribution over the \(z\)'s (\(\sum_z Q_i(z)=1,Q_i(z)\ge 0\)).Consider the following: \[ \begin{align} L(\theta)&amp;=\sum_{i=1}^mlog\sum_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)\\ &amp;=\sum_{i=1}^mlog\sum_{z^{(i)}}Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)}}\\ &amp;\ge\sum_{i=1}^m\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)}} \end{align} \] The last step of this derivation used Jensen's inequality. Specially, \(f(x)=log(x)\) is a convave function, since \(f^{&#39;&#39;}(x)=-1/x^2 &lt; 0\) over its domain \(x \in \Re^+\). Also, the term \[\sum_{z^{(i)}}Q_i(z^{(i)})\left[\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)}}\right]\] in the summation is just an expectation of the quantity \([p(x^{(i)},z^{(i)};\theta)/Q_i(z^{(i)})]\)with respect to \(z^{(i)}\) drawn according to the distrubtion given by \(Q_i\). By Jensen's inequality, we have \[f\left(E_{z^{(i)}\sim Q_i}\left[\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\right]\right)\ge E_{z^{(i)}\sim Q_i}\left[f\left(\frac{p(x^{(i)}),z^{(i)};\theta}{Q_i(z^{(i)})}\right)\right]\] where the &quot;\(z^{(i)}\sim Q_i\)&quot; subscripts above indicate that the expectations are with respect to \(z^{i}\) drawn from \(Q_i\). This allowed us to go from Equation(2) to Equation(3). Now for any of distributions \(Q_i\) the formula (3) gives a lower-bound on \(l(\theta)\). There're many possible choices for the \(Q_i\)'s. Which should we choose? Well, if we have some current guess \(\theta\) of the parameters, it seems natural to try to make the lower-bound tight at that value of \(\theta\). I.e., we'll make the inequality above hold with equality at our particular value of \(\theta\). (We'll see later how this enables us to prove that \(l(\theta)\) increase monotonically with successive iterations of EM.) To make the bound tight for a particular value of \(\theta\), we need for the step involving Jensen's inequality in our derivation above to hold with equality. For this to be true, we know it is sufficient that the expectation be taken over a &quot;constant&quot;-valued random variable. I.e., we require that \[\frac{p(x^{(i)}),z^{(i)};\theta}{Q_i(z^{(i)})}=c\] for some constant \(c\) that does not depend on \(z^{i}\). This is easily accomplished by choosing \[Q_i(z^{(i)})\propto p(x^{(i)},z^{(i)};\theta)\] Actually, since we know \(\sum_z Q_i(z^{(i)})=1\) (because it is a distribution), this further tells us that \[ \begin{align*} Q_i(z^{i})&amp;=\frac{p(x^{(i)},z^{(i)};\theta)}{\sum_z p(x^{(i)},z^{(i)};\theta)}\\ &amp;=\frac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)}\\ &amp;=p(z^{(i)}|x^{(i)};\theta) \end{align*} \] Thus, we simply set the \(Q_i\)'s to be the posterior distribution of the \(z^{(i)}\)'s given \(x^{(i)}\) and the setting of the parameters \(\theta\). Now, for this choice of the \(Q_i\)'s, Equation(3) gives a lower-bound on the loglikelihood \(l\) that we're trying to maximize. This is the E-step. In the M-step of the algorithm, we then maximize our formula in Equation (3) with respect to the parameters to obtain a new setting of the \(\theta\)'s. Repeatedly carrying out these steps gives us the EM algorithm, which is as follows: Repeat until convergence{ (E-step) For each \(i\), set \[Q_i(z^{(i)}):=p(z^{(i)}|x^{(i)};\theta)\] (M-step) Set \[\theta := \mathop{\arg\max}_{\theta} \sum_i \sum_{z^{(i)}} Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\] } How we know if this algorithm will converge? Well, suppose \(\theta^{(t)}\) and \(\theta^{(t+1)}\) are the parameters from two successive iterations of EM. We will now prove that \(l(\theta^{(t)})\le l(\theta^{(t+1)})\), which shows EM always monotonically improves the log-likelihood. The key to showing this result lies in our choice of the \(Q_i\)'s. Specially, on the iteration of EM in which the parameters had started out as \(\theta^{(t)}\), we would have chosen \(Q_i^{(t)}(z^{(i)}):=p(z^{(i)}|x^{(i)};\theta^{(t)}).\) We saw earlier that this choice ensures that Jensen's inequality, as applied to get Equation (3), holds with equality, and hence \[l(\theta^{(t)})=\sum_i \sum_{z^{(i)}} Q_i^{(t)}(z^{(i)})log \frac{p(x^{(i)},z^{(i)};\theta^{(t)})}{Q_i^{(t)}(z^{(i)})}\] The parameters \(\theta^{(t+1)}\) are then obtained by maximizing the right hand side of the equation above. Thus, \[ \begin{align} l(\theta^{(t+1)}) &amp;\ge \sum_i \sum_{z^{(i)}} Q_i^{(t)}(z^{(i)})log \frac{p(x^{(i)},z^{(i)};\theta^{(t+1)})}{Q_i^{(t)}(z^{(i)})}\\ &amp;\ge \sum_i \sum_{z^{(i)}} Q_i^{(t)}(z^{(i)})log \frac{p(x^{(i)},z^{(i)};\theta^{(t)})}{Q_i^{(t)}(z^{(i)})}\\ &amp;=l(\theta^{(t)}) \end{align} \] This first inequality comes from the fact that \[l(\theta) \ge \sum_i \sum_{z^{(i)}} Q_i(z^{(i)})log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\] holds for any values of \(Q_i\) and \(\theta\), and in particular holds for \(Q_i=Q_i^{(t)}\), \(\theta = \theta^{(t+1)}\). To get Equation (5), we used the fact that \(\theta^{(t+1)}\) is chosen explicitly to be \[\mathop{\arg\max}_{\theta} \sum_i \sum_{z^{(i)}} Q_i(z^{(i)})log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\] and thus this formula evaluated at \(\theta^{(t+1)}\) must be equal to or larger than the same formula evaluated at \(\theta^{(t)}\). Finally, the step used to get (6) was shown earlier, and follows from \(Q_i^{(t)}\) having been chosen to make Jensen's inequality hold with equality at \(\theta^{(t)}\). Hence, EM causes the likelihood to converge monotonically. In our description of the EM algorithm, we said we'd run it until convergence. Given the result that we just showed, one reasonable convergence test would be to check if the increase in \(l(\theta)\) between successive iterations is smaller than some tolerance parameter, and to declare convergence if EM is improving \(l(\theta)\) two slowly. Remark. if we define \[J(Q,\theta) = \sum_i \sum_{z^{(i)}} Q_i(z^{(i)})log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{( i)})}\] then we know \(l(\theta) \ge J(Q,\theta)\) from our previous derivation. The EM can also be viewed a coordinate ascent on J, in which the E-step maximizes it with respect to Q, and the M-step maximizes it with respect to \(\theta\). Mixture of Gaussians Suppose that we are given a training set \(\{x^{(1)},\cdots,x^{(m)}\}\) as usual. Since we are in the unsupervised learning setting, these points do not come with any labels. We wish to model the data by specifying a joint distribution \(p(x^{(i)},z^{(i)})=p(x^{(i)}|z^{(i)})p(z^{(i)})\). Here, \(z^{(i)}\sim\) Multinomial(\(\phi\)) (where \(\phi \ge 0,\sum_{j=1}^k \phi_j = 1\), and the parameter \(\phi_j\) gives \(p(z^{(i)}=j)\)), and \(x^{(i)}|z^{(i)}=j \sim \mathcal{N}(\mu_j,\Sigma_j)\). We let \(k\) denote the number of values that the \(z^{(i)}\)'s can take on. Thus, our model posits the each \(x^{(i)}\) was generated by randomly choosing \(z^{(i)}\) from \(\{1,\cdots,k\}\), and then \(x^{(i)}\) was generated by randomly choosing \(z^{(i)}\) from \({1,\cdots,k}\), and then \(x^{(i)}\) was drawn from one of \(k\) Gaussains depending on \(z^{(i)}\). This is called the mixture of Gaussians model. Also, note that the \(z^{(i)}\)'s are latent random variables, meaning that they're hidden/unobserved. This is what will make our estimation problem difficult. The parameters of our model are thus \(\phi,\mu\) and \(\Sigma\). To estimate them, we can wirte down the likelihood of our data: \[ \begin{align*} l(\phi,\mu,\Sigma)&amp;=\sum_{i=1}^mlogp(x^{(i)};\phi,\mu,\Sigma)\\ &amp;=\sum_{i=1}^mlog\sum_{z^{(i)}=1}^k p(x^{(i)}|z^{(i)};\mu,\Sigma)p(z^{(i)};\phi) \end{align*} \] However, if we set to zero the derivatives of this formula with respect to the parameters and try to solve, we'll find that it is not possible to find the maximum likelihood estimates of the parameters in closed form. The random variables \(z^{(i)}\) indicate which of the \(k\) Gaussians each \(x^{(i)}\) had come from. Note that if we knew what the \(z^{(i)}\)'s were, the maximum likelihood problem would have been easy. Specifically, we could then write down the likelihood as \[l(\phi,\mu,\Sigma)=\sum_{i=1}^mlogp(x^{(i)}|z^{(i)};\mu,\Sigma)+logp(z^{(i)};\phi)\] Maximizing this with respect to \(\phi,\mu\) and \(\Sigma\) gives the parameters: \[ \begin{align*} \phi_j &amp;= \frac{1}{m} \sum_{i=1}^m 1\{z^{(i)}=j\}\\ \mu_j &amp;= \frac{\sum_{i=1}^m 1\{z^{(i)}=j\}x^{(i)}}{\sum_{i=1}^m 1\{z^{(i)}=j\}}\\ \Sigma_j &amp;= \frac{\sum_{i=1}^m 1\{z^{(i)}=j\}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^m 1\{z^{(i)}=j\}} \end{align*} \] Indeed, we see that if the \(z^{(i)}\)'s were known, then maximum likelihood estimation becomes nearly identical to what we had when estimating the parameters of the Gaussian discriminat analysis model, except that here the \(z^{(i)}\)'s playing the role of the class labels. However, in our density estimation problem, the \(z^{(i)}\)'s not known.What can we do? The EM algorithm is an iterative algorithm that has two main steps. Applied to our problem, in the E-step, it tries to &quot;guess&quot; the values of the \(z^{(i)}\)'s. In the M-step, it updates the parameters of our model based on our guesses. Since in the M-step we are pretending that the guesses in the first part were correct, the maximization becomes easy. Here's the algorithm: Repeat until convergence: { (E-step) For each \(i,j\), set \[w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)\] (M-step) Update the parameters: \[ \begin{align*} \phi_j &amp;:= \frac{1}{m}\sum_{i=1}^m w_j^{(i)}\\ \mu_j &amp;:= \frac{\sum_{i=1}^m w_j^{(i)}x^{(i)}}{\sum_{i=1}^m w_j^{(i)}}\\ \Sigma_j &amp;:= \frac{\sum_{i=1}^m w_j^{(i)}(x^{(i)}-u_j)(x^{(i)}-u_j)^T}{\sum_{i=1}^m w_j^{(i)}} \end{align*} \] In the E-step, we calculate the posterior probability of our parameters the \(z^{(i)}\)'s, given the \(x^{(i)}\) and using the current setting of our parameters. I.e., using Bayes rule,we obtain: \[p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)=\frac{p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{\sum_{l=1}^k p(x^{(i)}|z^{(i)}=l;\mu,\Sigma)p(z^{(i)}=l;\phi)}\] Here, \(p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)\) is given by evaluating the density of a Gaussian with mean \(\mu_j\) and covariance \(\Sigma_j\) at \(x^{(i)}\); \(p(z^{(i)}=j;\phi)\) is given by \(\phi_j\), and so on. The values \(w_j^{(i)}\) calculated in the E-step represent our &quot;soft&quot; guesses for the value of \(z^{(i)}\). Also, you should constract the update in the M-step with the formulas we had when the \(z^{(i)}\)'s were known exactly. They are identical, except that instead of the indicator functions &quot;\(1\{z^{(i)}=j\}\)&quot; indicating from which Gaussian each datapoint had come, we now instead have the \(w_j^{(i)}\)'s. The EM-algorithm is also reminiscent of the K-means clustering algorithm, expcept instead of the &quot;hard&quot; cluster assignments \(c(i)\), we instead have the &quot;soft&quot; assignments \(w_j^{(i)}\). Similar to K-means, it is also susceptible to local optima, so reinitializing at several different initial parameters may be a good idea. Armed with our general definition of the EM algorithm, let's go back to the example of fitting the parameters \(\phi\), \(\mu\) and \(\Sigma\) in a mixture of Gaussians. The E-step is easy. Following our algorithm derivation above, we simply calculate \[w_j^{(i)}=Q_i(z^{(i)}=j)=P(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma).\] Here, &quot;\(Q_i(z^{(i)}=j)\)&quot; denotes the probability of \(z^{(i)}\) taking the value \(j\) under the distribution \(Q_i\). Next, in the M-step, we need to maximize, with respect to our parameters \(\phi,\mu,\Sigma,\) the equality \[ \begin{align} \sum_i \sum_{z^{(i)}} Q_i(z^{(i)})&amp;log \frac{p(x^{(i)},z^{(i)};\phi,\mu,\Sigma)}{Q_i(z^{(i)})}\\ &amp;= \sum_{i=1}^m \sum_{z^{(i)}} Q_i(z^{(i)}=j)log \frac{p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{Q_i(z^{(i)}=j)}\\ &amp;=\sum_{i=1}^m \sum_{j=1}^k w_j^{(i)}log\frac{\frac{1}{(2\pi)^{n/2}|\Sigma_j|^{1/2}}exp(-\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j))\dot\phi_j}{w_j^{(i)}} \end{align} \] Let's maximize this with respect to \(\mu_l\). If we take the derivative with respect to \(\mu_l\), we find \[ \begin{align} \nabla_{\mu_l} \sum_{i=1}^m \sum_{j=1}^k w_j^{(i)}&amp;log\frac{\frac{1}{(2\pi)^{n/2}|\Sigma_j|^{1/2}}exp(-\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j))\dot\phi_j}{w_j^{(i)}}\\ &amp;= -\nabla_{\mu_l} \sum_{i=1}^m \sum_{j=1}^k w_j^{(i)}\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j))\\ &amp;=\frac{1}{2}\sum_{i=1}^m w_l^{(i)}\nabla_{\mu_l}2\mu_l^T\Sigma_l^{-1}x^{(i)}-\mu_l^T\Sigma_l^{-1}\mu_l\\ &amp;=\sum_{i=1}^m w_l^{(i)}(\Sigma_l^{-1}x^{(i)}-\Sigma_l^{-1}\mu_l) \end{align} \] setting this to zero and solving for \(\mu_l\) therefor yields the updata rule \[\mu_l := \frac{\sum_{i=1}^m w_l^{(i)}x^{(i)}}{\sum_{i=1}^m w_l^{(i)}}\] Let's do one more example, and derive the M-step updata for the parameters \(\phi_j\). Grouping together only the terms that depend on \(\phi_j\), we find that we need to maximize \[\sum_{i=1}^m \sum_{j=1}^k w_j^{(i)}log\phi_j\] However, there is an additional constraint that the \(\phi_j\)'s sum to 1, since they represent the probabilities \(\phi_j = p(z^{(i)}=j;\phi)\). To deal with the constraint that \(\sum_{j=1}^k \phi_j = 1\), we construct the Lagrangian \[L(\phi)=\sum_{i=1}^m \sum_{j=1}^k w_j^{(i)}log\phi_j + \beta(\sum_{j=1}^k \phi_j-1),\] where \(\beta\) is the Lagrange multiplier. Taking derivatives, we find \[\frac{\partial}{\partial\phi_j}L(\phi)=\sum_{i=1}^m\frac{w_j^{(i)}}{\phi_j}+1\] Setting this to zero and solving, we get \[\phi_j = \frac{\sum_{i=1}^m w_j^{(i)}}{-\beta}\] I.e., \(\phi_j\propto \sum_{i=1}^m w_j^{(i)}\).Using the constraint that \(\sum_j\phi_j=1\), we easily find that \(-\beta = \sum_{i=1}^m \sum_{j=1}^k w_j^{(i)}=\sum_{i=1}^m 1 = m\). (This used the fact that \(w_j^{(i)}=Q_i(z^{(i)}=j)\), and since probabilities sum to 1, \(\sum_j w_j^{(i)}=1\).) We therefore have our M-step updates for the parameter \(\phi_j\): \[\phi_j := \frac{1}{m}\sum_{i=1}^m w_j^{(i)}\] The derivation for M-step updates to \(\Sigma_j\) are also entirely straightforward.Look at it as below. \[ \begin{align*} \nabla_{\Sigma_l} \sum_{i=1}^m \sum_{j=1}^k w_j^{(i)}&amp;log\frac{\frac{1}{(2\pi)^{n/2}|\Sigma_j|^{1/2}}exp(-\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j))\dot\phi_j}{w_j^{(i)}}\\ &amp;=\nabla_{\Sigma_l}\sum_{i=1}^m\sum_{j=1}^k w_j^{(i)}\left\{-\frac{1}{2}log |\Sigma_j|-\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j)\right\}\\ &amp;=\nabla_{\Sigma_l}\sum_{i=1}^m w_l^{(i)}\left\{log|\Sigma_l|+(x^{(i)}-\mu_l)^T\Sigma_l^{-1}(x^{(i)}-\mu_l)\right\} \end{align*} \] The \(|S|\) is the determinant of \(S\), we express it as \(det(S)\). Let \(S=\Sigma\), \(y_i=x^{(i)}-\mu_l\), \(J(S)=\sum_{i=1}^m w^{(i)}\left\{log det(S)-y_i^TS^{-1}y_i\right\}, (w^{(i)}&gt;0, y_i \in \mathbb{R}^m, S\in \mathbb{R}^{m \times m})\). We know that \(det(S)I = S Adj(S)\), \(\frac{\partial det(S)}{\partial S}=det(S)S^{-1}\),\((S^{-1})^{\prime}=-S^{-1}S^{\prime}S^{-1}\), \(\frac{\partial S}{\partial S}=I\), you'll find \[ \frac{\partial log det(S)}{\partial S}=\frac{det(S)S^{-1}}{det(S)}=S^{-1}\] \[\frac{\partial J(S)}{\partial S}=\sum_{i=1}^m w^{(i)}S^{-1}-\sum_{i=1}^m w^{(i)}y_iy_i^T(-S^{-1}IS^{(-1)})=0\] \[ \begin{align*} \sum_{i=1}^m w^{(i)}S^{-1}&amp;=\sum_{i=1}^m w^{(i)}y_iy_i^TS^{-1}IS^{(-1)}\\ \sum_{i=1}^m w^{(i)}&amp;=\sum_{i=1}^m w^{(i)}y_iy_i^TS^{-1}\\ S&amp;=\frac{\sum_{i=1}^m w^{(i)}y_iy_i^T}{\sum_{i=1}^m w^{(i)}} \end{align*} \]]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F08%2F26%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new "My New Post" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy simple inline \(a=b+c\) More info: Deployment]]></content>
  </entry>
</search>
